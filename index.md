
 SHANMUGAM E
 +91-790446431
 shan2cog@gmail.com

Objective:

To be a Sucessful professional and to utilize the best of abilities for the company welfare thereby making handsome profit to the company. 	 	 	

Professional Summary:

Overall 3 plus year of IT experience which includes strong background knowledge with Big data technologies
Having Good Experience in Hadoop,Hive,Sqoop, Hbase,Spark, scala
Handling the Semi structured Data and  Strucutred data.
Hands on Experience in spark streaming knowledge with kafka and flume.
Knowledge in programming of Java using IDE Eclipse and Intelij.
Well versed on configurations and development on Rest API.
High level understanding and integration other Microsoft source with Big data technology. 
Proficiency in developing web based applications, experienced in J2EE technologies and PHP Rest API,MEAN.

Academic Summary:

Bachelor of Engineering in ComputerScience Engineering from
Meenakshi College of Engineering, Anna University,2013

Professional Summary:

Worked as a Software Engineer in Viven Technologies at Chennai From
Nov 2013 –April 2016.

Worked as a Software Engineer in CVIAC Consulting Pvt LTD at Chennai From Jun 2016 –April 2017.
Technical Exposure:

Hadoop Eco-System Tools

HDFS,PIG,HIVE,SQOOP,HBASE. 
Hadoop Versions 

Apache Hadoop 2.7.3

Programming Languages

JAVA

Frameworks

REST API, HDFS, STRUTS 1.2

RDBMS

Oracle 10g,MySql.

Operating Systems

Windows,Linux(Ubuntu).



Project :1

Project
Click Stream Analysis
Components
Hive,Sqoop,Pig
Tech. Environment
Hadoop 
Database
MSSql, Oracle SQL
Duration
May 2017-tilldate
 Team Size
3

Description:
This application integrates Hive Queries in HQL for analysing clickstream data and obtaining different conclusions based on predefined conditions. Hive Queries Developed for analysing and aggregating data about navigation behavior of webpage visitors in terms of order of pages visited and sequence and similar analytical task.

Role & Responsibilities:
Experienced on loading and transforming of large sets of structured and semi structured data. 
Develop Hive queries for the analysis of data.
Created Hive queries to set performance tuning parameters to increase overall performance.
Migrating the data to MySql from Hive using Sqoop.
Writing the design and technical documentation of the solution for the client.
Project :2
Project
Data Ingestion using Hadoop
Components
Hive,Sqoop
Tech. Environment
Hadoop 
Database
MSSQL ,Oracle SQL
Duration
Jun 2016-Apr 2017
 Team Size
3

Description:
	
    Data Ingestion using Hadoop framework,the common platform for data ingestion which will load data from ERP system databases to Hadoop.it will move the data between external system to hadoop.This includes ingesting data into  Hadoop from system such as relational databases and extracting data from hadoop for ingestion into external systems and then used specific tools for data ingestion with hadoop such as sqoop and hive.

Role & Responsibilities:

Involved in requirement analysis,design and development.
Developed data ingestion shell scripts to load data from source systems(MySql,SQL server) to hve using sqoop import commands.
Import and Export data into hdfs and hive using sqoop.
Load and transform large sets ofpoc
structured and semi structured data.
Involved in creating hive tables,loading data into hive tables and Writing hive queries.
Project
Project
Book Rating Data
Components
Hive,Sqoop
Tech. Environment
Hadoop ,Linux
Database
MySql,SQL
Duration
Nov 2015-Mar 2016
Project Team Size
3
                                                                                                                           
Description:
The Purpose of the project involves in doing basic analysis on Book Rating data using
Apache Hive and Apache Pig.Once the data is analysed using Hive queries and Pig
commands ,the summarized data is stored in HDFS and Mapped to Hive Tables for reporting
purpose.                                                                                                                                                  

Role & Responsibilities:
Involved in requirement analysis,design and development.
Involved in writing Hive Queries to load and process data into HDFS.
Import and Export data into HDFS and hive using sqoop.
Load and transform large sets of structured and semi structured data.
Involved in creating hive tables,loading data into hive tables and Writing hive queries.
Project: 3
Project
Knoinf(www.knoinf.in)
Tech. Environment
PHP,Ajax,Jquery
Database 
Mysql.
Duration 
Jan 2015-Oct 2015.
Project Team Size
 3

Description:
Knoinf uses its own unique model to categorize training providers based on their best practices, accolades and achievements. Knoinf creates the right transformation path for students/professionals who are seeking to learn technologies via all modes of learning.
Role & Responsibilities:

Developing the UI and Functionalities.
Test and Debug the Screens and Functionalities.
Project:4
Project
Claims Management System
Technology
Struts 1.2, Hibernate2.3
Database 
Oracle 10g
Duration 
Mar 2014 to Dec 2014
Project Team Size
7

Description:
   Claims Management System is an application used by AFRS (Anti-fraud Recovery Solutions) the fraud investigation of prospective claims which primarily aims at automating the case workflow for investigators with functionalities such as performing case tasks, generating reports and letters. Tasks include conducting interviews, license checks, analyzing medical records and sending letters for members and providers. 

Roles and Responsibilites:

Being one of the team member, worked Pro-actively and played vital role in collecting and consolidating client requirements.
Involved in Analysis, Development and Unit Testing of CMS case management using Struts framework, JSP and Hibernate.
Worked in Resolving and fixing errors on a daily basis occurs in production systems.
Involved improving the performance in mainly used Search Screens like Provider and Member Search.
	

                                                                                            	
